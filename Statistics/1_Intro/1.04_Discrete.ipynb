{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistics\n",
    "## Chapter 1.4 - Discrete Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "- Probability distribution\n",
    "    - A mathematical function that models/ describes the probability of a result occuring\n",
    "    - The probability that the random variable $X$ takes on the value $x$, $P(X=x)$\n",
    "        - The function is normally defined using an equation, but it could be defined using anything as 1 input corresponds to 1 output (such as a table or a histogram)\n",
    "\n",
    "## Notation\n",
    "- Events with random variable\n",
    "    - The event can be defined using random variables, a particular outcome, and inequalities\n",
    "    - e.g. $X=9, X\\le9, X=x, X\\le x$\n",
    "- Probability with random variable\n",
    "    - Same as probability of an event defined in chapter 1.1 expect the event is defined using a random variable instead of a set\n",
    "    - e.g. $P(X = 9)$, $P(X \\le 9)$, $P(X = x)$, $P(X \\le x)$\n",
    "    - $P(X = x)$ will sometimes be shorten to $p(x)$ or $p_X(x)$\n",
    "- Probability distribution\n",
    "    - $P(X = x)$,  $p(x)$, or $p_X(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Mass Function\n",
    "The probability distribution of a descrete random variable is also called a probability mass function (pmf). Typically, we only explicitly define the regions where the probability is $>0$. Everything that is not explicitly defined has a probability of $0$.\n",
    "\n",
    "Recall the axioms of probability:\n",
    "- $P(S) = 1$  \n",
    "- $P(A) \\ge 0$  \n",
    "- $P(S) = \\sum_{i=1}^n P(A_i)=1$, if $A_i$ are mutually exclusive for all $i$   \n",
    "\n",
    "Unsurprisingly, these also apply to the probability mass function. Lets define set $A$ to be events where $X = x$. Then using substitution, we get $P(X = x) \\le 1$. Lets define $A_i$ to be events events $X = x_i$. Clearly these events are mutually exclusive because an outcome cannot have 2 different results. Then using substitution again, we get $\\sum_{x_i} P(X = x_i) = P(S) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Distribution Function\n",
    "\n",
    "The cumulative distribution function (cdf) is the probability the random variable $X$ is less than or equal to a particular value $x$, and is typically denoted as $F(x)$. The cdf also just so happens to model the quantile of the distribution.\n",
    "- $F(x)=P(X \\le x)$\n",
    "\n",
    "We can also use the cdf to calculate the probability of $X$ being between the intervals $[x_1, x_2]$\n",
    "- $P(x_1 \\le X \\le x_2) = F(x_2) - F(x_1)$\n",
    "\n",
    "To calculate the cdf for a discrete random variable, you would calculate all of the pmfs from $-\\infty$ to $x$ and add them together\n",
    "- $F(x)=P(X \\le x) = \\sum _{y \\le x} p(y)$ \n",
    "\n",
    "Using the properties of a pmf, we can derive the properties of a cdf.\n",
    "- $\\lim_{x \\to -\\infty} F(x) = 0$\n",
    "    - No numerical result can have a value of $\\le -\\infty$. So if the set of events where $X \\le -\\infty$ must be the empty set, thus $P(X \\le \\infty) = 0$.\n",
    "- $\\lim_{x \\to \\infty} F(x) = 1$\n",
    "    - All numerical result must have a value of $\\le -\\infty$. So if the set of events where $X \\le \\infty$ must be the entire sample space, thus $P(X \\le \\infty) = 1$.\n",
    "- If $x_1 < x_2$, then $F(x_1) \\le F(x_2)$\n",
    "    - $F(x)$ is a nondecreasing function.\n",
    "    - This is because $p(x) \\ge 0$ for all values of $x$. It is impossible to decrease the cdf by adding more numbers $\\ge0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Value\n",
    "Expected value is the theoretically mean for a probability distribution. This is normally notated with a $E$ followed by the random variable in parenthese, e.g. $E(X)$, or the greek symbol mu, $\\mu$. The equation for expected value of a probability mass function is the following:\n",
    "- (1) $E(X) =\\mu= \\sum_x xP(X=x)$\n",
    "\n",
    "Lets see how this relates to the mean equation.\n",
    "- expected value equation\n",
    "    - $E(X) = \\sum_x xp(x)$  \n",
    "- rewrite $p(x)$ using the probability equation\n",
    "    - $= \\sum_x x\\frac{|X=x|}{|S|}$\n",
    "- move $|S|$ outside the summation because it is a constant\n",
    "    - $= \\frac{1}{|S|} \\sum_x x|X=x|$\n",
    "        - $|X=x|$ can be intrepted as the number of times $X=x$ occurred\n",
    "        - $\\sum_x x|X=x|$ can be intrepted as grouping the results together and finding the sum of those first. Then summing everything together after.\n",
    "        - $\\frac{1}{|S|}$ can be intrepted as dividing everything by the total\n",
    "    - The overall intreptation is the same as the mean equation, sum up all of the results and dividing it by the total number of events. However, the expected value equation sum the same results first before summing everything together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Expected Value\n",
    "Expected value of a probability distribution will always yield the same result. This means that expected value has no randomness (behaving like a random variable, potentially having a different result) therefore it is a constant. This differs from a function of random variables, such as sample mean, $\\bar y$. This is because using a different set of random variables may yield a different value.\n",
    "\n",
    "### Equations\n",
    "- (2) $E(g(X)) = \\sum_x g(x)p(x)$\n",
    "    - You would think that $E(g(X)) = \\sum_x g(x)p(g(x))$\n",
    "    - $p(g(x)) = p(x)$\n",
    "        - This is because $x$ and the $g(x)$ are refering to the same event, but with different result values.\n",
    "- (3) $E(c) = c$\n",
    "    - $c$ is a constant\n",
    "- (4) $E(cX) = cE(X)$\n",
    "- (5) $E(X + Y) = E(X) + E(Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance\n",
    "The theoretical variance is normally notated as $Var$ or $V$ followed by the random variance in parenthese, e.g. $Var(X)$ or $V(X)$. The equation for the theoretical variance is defined as the following:\n",
    "- (6) $Var(X) = E[(X - \\mu)^2]$ \n",
    "    - Using equation 2, we can that $Var(X)= \\sum_x (x-\\mu)^2p(x)$\n",
    "\n",
    "Since $\\mu$ can be interpreted as the \"mean\" of the distribution. The variance equation can be interpreted as the average of the squared difference between the result and the mean. Since the difference is squared, the following must be true:\n",
    "- $Var(X) \\ge 0$\n",
    "\n",
    "Notice that the sample variance equation is not the average of the squared difference the result and the mean. Instead of dividing by $n$, it divides by $n-1$.  \n",
    "- $\\frac{1}{n-1} \\sum_{i=1}^n(y_i-\\bar{y}^2)$\n",
    "\n",
    "This is because of several reasons:\n",
    "- Using $\\frac{1}{n}$ yields a biased estimator, covered in a later chapter$\n",
    "- Has $n-1$ degrees of freedom\n",
    "    - Degrees of freedom refers to the number of values that are free to vary\n",
    "    - The equation uses $\\bar y$, which uses the data to calculate. This means $\\bar y$ holds information about the data. So we have to factor that in when calculating variance.\n",
    "        - $\\bar y$ has the following property: $\\sum_{i=1}^n (y_i - \\bar y) = 0$\n",
    "        - This means that n - 1 number of data can be anything, but the nth data must be a value that satisfy the constaint above.\n",
    "\n",
    "We can also rearrange the equation into:\n",
    "- (7) $Var(X) = E(X^2) - \\mu^2 = \\sum_x (x)^2p(x) - \\mu^2$\n",
    "    - $Var(X) = E[(X - \\mu)^2] = E[X^2 - 2X\\mu + \\mu^2] = E(X^2) - E(2X\\mu) + E(\\mu^2)$\n",
    "        - Note: $\\mu$ is a constant\n",
    "    - $ = E(X^2) - 2\\mu E(X) + E(\\mu^2) = E(X^2) - 2\\mu^2 + E(\\mu^2) = E(X^2) - \\mu^2$\n",
    "- This is a useful property of variance, I don't think this has great interpretation value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Variance\n",
    "Similar to expected value, variance is a constant while sample variance is a random variable.\n",
    "\n",
    "### Equations\n",
    "- (8) $Var(c) = 0$\n",
    "    - $c$ is a constant\n",
    "- (9) $Var(X + c) = Var(X)$\n",
    "    - $Var(X+c) = E[(X+c - E(X+c))^2] = E[(X+c - E(X) - c)^2] = E[(X - E(X))^2] = Var(X)$\n",
    "    - This can be interpreted as the spread of the data staying the same\n",
    "- (10) $Var(aX) = a^2Var(X)$\n",
    "    - $Var(aX) = E[(aX - a\\mu)^2] = E[a^2(X - \\mu)^2] = a^2 E[(X - \\mu)^2] = a^2Var(X)$\n",
    "- (11) $Var(X + Y) = Var(X) + Var(Y)$, if $X$ and $Y$ are independent\n",
    "- (12) $Var(aX + bY) = a^2Var(X) + b^2Var(Y) + abCov(X, Y)$\n",
    "    - $Cov$ will be discussed in a later chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Random Variable\n",
    "The simplest experiment we can have is one with 2 possible outcomes. Typically, the outcomes are referred to as \"success\", the one (most) related to the question of interest, and \"failure\". Mathematically, the outcomes are encoded as 1 or 0 respectively. This type of random variable is called a Bernoulli random variable or a Bernoulli trail (name after Jacob Bernoulli). The remain discrete random variables introduced in this chapter will be derived from the Bernoulli.\n",
    "\n",
    "Suppose probability of success, $P(X=1)$, is some constant $p$. Then $P(X=0)$ must be $1-p$ because it is the complement of $X=1$. We can combine them together to form the following pmf:\n",
    "- (13) $p(x) = \\begin{cases} p, \\quad \\text{if }x=1 \\\\ 1-p, \\quad \\text{if }x=0 \\end{cases}$  \n",
    "\n",
    "Using the pmf, we can derive the expected value and variance.  \n",
    "### Expected Value and Variance\n",
    "- (14) $E(X) = p$\n",
    "    - $E(X) = \\sum_x xp(x) = 0(1-p) + 1*p = p$  \n",
    "- (15) $Var(X) = p(1-p)$\n",
    "    - $Var(X) =\\sum_x (x-\\mu)^2p(x) = \\sum_x (x-p)^2p(x) = (1-p)^2p + (0-p)^2(1-p)$\n",
    "    - $= p - 2p^2 +p^3 + p^2 - p^3 = p - p^2 = p(1-p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial\n",
    "Binomial random variable models the number of successes in $n$ iid Bernoulli trials. \n",
    "\n",
    "Suppose the random variable $X$ above is a coin flipping experiment, with $X=1$ if the coin is heads and $X=0$ otherwise (sometimes shorten to o.w.). If we repeat the coin flipping experiment $n$ times, then we would generate a sequence of random variables, $X_1, X_2, X_3, ..., X_n$. These random variables are said to be independent because the outcome of one experiment will not effect the outcome of another, and identically distributed because we are using the same coin. We are assuming that the coin behavior does not change between experiments, therefore having the same probability distribution (same type of random variables with same parameters). Independent and identically distributed is typically shorten to iid. Since success is encoded as a 1 for a Bernoulli, binomial can be seen as the sum of $n$ iid Bernoulli trials.\n",
    "\n",
    "It is fairly simple to derive the binomial pmf.\n",
    "- Given: $n$ number of Bernoulli trails, $x$ number of successes\n",
    "- We know that the probability of success for a Bernoulli is $p$. So the probability of x number of success is $p^x$.\n",
    "- We know that the probability of failure for a Bernoulli is $1-p$ and the number of failure is $n-x$. So the probability of $n-1$ number of failure is $(1-p)^{n-x}$.\n",
    "- There are many ways to have $x$ number of successes in $n$ trails. We can calculate the number of ways by using combinations, $\\binom nx$.\n",
    "\n",
    "By combining everything above, we have the following pmf:\n",
    "- (16) $p(x) = \\binom n x p^x (1-p)^{n-x}$, for $n \\in \\mathbb{N}, \\quad p \\in [0,1], \\quad x \\in \\{0, 1, 2, ..., n\\} $\n",
    "\n",
    "### Notation\n",
    "- $X \\sim B(n, p)$\n",
    "\n",
    "### Expected Value and Variance\n",
    "We could derive both of these from the pmf. However, it is significantly easier to derive them using a sequence of Bernoulli random variables, $Y_1, ..., Y_n$, instead.\n",
    "- (17) $E(X) = np$\n",
    "    - $E(X) = E(Y_1 + Y_2 + ... + Y_n) = E(Y_1) + ... + E(Y_n) = np$\n",
    "- (18) $Var(X) = np(1-p)$\n",
    "    - $Var(X) = Var(Y_1 + Y_2 + ... + Y_n) = Var(Y_1) + ... + Var(Y_n) = np(1-p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson\n",
    "\n",
    "Poisson (named after SimÃ©on Denis Poisson) is used to model the number of times something occurs over a fixed length of time, area, or volume.\n",
    "\n",
    "Imagine we are trying to model the number of car accidents over a year. We can try to model this as a binomial. We can split the year into $n$ intervals, and each interval will be a Bernoulli trail on whether a car accident occurred. We cannot have multiple car accident during the same time intervals because then it wont be a Bernoulli. So we have to have $n=\\infty$ for the probability of multiple car accident during the same intervals be $0$. Now we have the following equation:\n",
    "- $\\lim_{n \\to \\infty} \\binom n x p^x (1-p)^{n-x}$  \n",
    "\n",
    "Now we have another problem, $p$ must equal $0$ or else $n*p$ (expected value of binomial) will blow up to $\\infty$. Setting $p=0$ will cause our pmf to collapse to 0. Lets set $\\lambda=np$. Then we can subsitute $p$ with $\\frac{\\lambda}{n}$.\n",
    "- $\\lim_{n \\to \\infty} \\binom n x \\Big( \\frac{\\lambda}{n}\\Big) ^x (1- ( \\frac{\\lambda}{n}) ^{n-x}$  \n",
    "\n",
    "We can simplify this to get our new pmf, the pmf of a Poisson:\n",
    "- (19) $p(x) = \\lim_{n \\to \\infty} \\binom n x \\frac{\\lambda}{n}^x (1-\\frac{\\lambda}{n})^{n-x} = \\frac{\\lambda^x e^\\lambda}{x!}$, for $\\lambda >0, \\quad x \\in \\mathbb{N}$\n",
    "    - $\\lim_{n \\to \\infty} \\binom n x \\Big( \\frac{\\lambda}{n}\\Big) ^x (1- \\Big( \\frac{\\lambda}{n})\\Big) ^{n-x} = \\lim_{n \\to \\infty} \\frac{n!}{(n-x)!x!} \\Big( \\frac{\\lambda}{n}\\Big) ^x (1-  \\frac{\\lambda}{n}) ^{n-x} = \\lim_{n \\to \\infty} \\frac{n!\\lambda^x}{(n-x)!x!n^x} \\lim_{n \\to \\infty} (1-  \\frac{\\lambda}{n}) ^{n-x}$\n",
    "        - $\\lim_{n \\to \\infty} (1-  \\frac{\\lambda}{n}) ^{n-x} = \\lim_{n \\to \\infty} (1-  \\frac{\\lambda}{n}) ^{n} = e^{-\\lambda}$\n",
    "    - $= \\lim_{n \\to \\infty} \\frac{n!\\lambda^x}{(n-x)!x!n^x} e^{-\\lambda} = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\lim_{n \\to \\infty} \\frac{n!}{(n-x)!n^x} = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\lim_{n \\to \\infty} \\frac{n!}{(n-x)!} \\lim_{n \\to \\infty} \\frac{1}{n^x}$\n",
    "        - $\\lim_{n \\to \\infty} \\frac{n!}{(n-x)!} = \\lim_{n \\to \\infty} n^x$\n",
    "    - $= \\frac{\\lambda^x e^{-\\lambda}}{x!} \\lim_{n \\to \\infty} n^x \\lim_{n \\to \\infty} \\frac{1}{n^x} = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\lim_{n \\to \\infty} \\frac{n^x}{n^x} = \\frac{\\lambda^x e^{-\\lambda}}{x!}$\n",
    "\n",
    "### Notation\n",
    "- $X \\sim Pois(\\lambda)$\n",
    "\n",
    "### Expected Value and Variance\n",
    "- (20) $E(X) = \\lambda$\n",
    "- (21) $Var(X) = \\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others\n",
    "There are many other discrete random variables. Here is a list of some other common ones and their purpose. You can look up their pmf, expected value, variance, etc.\n",
    "- Geometric\n",
    "    - Models the number of Bernoulli trials needed to reach the first success.\n",
    "- Negative Binomial\n",
    "    - Models the number of Bernoulli trials needed to reach r number of success.\n",
    "- Hypergeometric\n",
    "    - Models the number of successful draws from a population of size $N$ with $K$ number of objects of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations\n",
    "Expected Value  \n",
    "1) $E(X) =\\mu= \\sum_x xP(X=x)$  \n",
    "2) $E(g(X)) = \\sum_x g(x)p(x)$  \n",
    "3) $E(c) = c$  \n",
    "4) $E(cX) = cE(X)$  \n",
    "5) $E(X + Y) = E(X) + E(Y)$  \n",
    "\n",
    "Variance  \n",
    "6) $Var(X) = E[(X - \\mu)^2]$   \n",
    "7) $Var(X) = E(X^2) - \\mu^2 = \\sum_x (x)^2p(x) - \\mu^2$  \n",
    "8) $Var(c) = 0$  \n",
    "9) $Var(X + c) = Var(X)$  \n",
    "10) $Var(aX) = a^2Var(X)$  \n",
    "11) $Var(X + Y) = Var(X) + Var(Y)$, if $X$ and $Y$ are independent  \n",
    "12) $Var(aX + bY) = a^2Var(X) + b^2Var(Y) + abCov(X, Y)$  \n",
    "\n",
    "Bernoulli  \n",
    "13) $p(x) = \\begin{cases} p, \\quad \\text{if }x=1 \\\\ 1-p, \\quad \\text{if }x=0 \\end{cases}$  \n",
    "14) $E(X) = p$  \n",
    "15) $Var(X) = p(1-p)$\n",
    "\n",
    "Binomial  \n",
    "16) $p(x) = \\binom n x p^x (1-p)^{n-x}$, for $n \\in \\mathbb{N}, \\quad p \\in [0,1], \\quad x \\in \\{0, 1, 2, ..., n\\} $  \n",
    "17) $E(X) = np$  \n",
    "18) $Var(X) = np(1-p)$\n",
    "\n",
    "Poisson  \n",
    "19) $p(x) = \\frac{\\lambda^x e^\\lambda}{x!}$, for $\\lambda >0, \\quad x \\in \\mathbb{N}$  \n",
    "20) $E(X) = \\lambda$  \n",
    "21) $Var(X) = \\lambda$  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}