{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistics\n",
    "## Chapter 1.6 - Continuous Random Variables\n",
    "Equations and properties for continuous random variables are very similar to discrete random variables. As a general rule of thumb, discrete random variables uses summations in their equations while continuous random variables uses integration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Distribution Function\n",
    "\n",
    "The definition of a cumulative distribution function for a continuous random variable is the same as the discrete case, $P(X \\le x)$, and has the exact some properties. It can also be interpreted as modeling the quantile of the distribution. The main difference is the cdf of a continuous distribution is a continuous function while the cdf of a discrete distribution is a stepwise function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Density Function\n",
    "### Definition\n",
    "A probability distribution of a continuous random variable is also called a probability density density (pdf) and is defined as the following:\n",
    "- (1) $f(x)=\\frac{dF(x)}{dx}$ wherever the derivative exists\n",
    "\n",
    "Since a cdf is nondecreasing function, we also know that $f(x)\\ge0$.  \n",
    "\n",
    "Based on the definition of a pdf, the cdf must be equal to the following:\n",
    "- (2) $F(x)=\\int_{-\\infty}^{x}f(t)dt$\n",
    "\n",
    "This would also mean we can find the probability of $X$ being between the interval $[x_1, x_2]$ using a pdf:\n",
    "- (3) $P(x_1 \\le X \\le x_2) = \\int_{x_1}^{x_2}f(t)dt$\n",
    "    - $P(x_1 \\le X \\le x_2) = F(x_2) - F(x_1) = \\int_{-\\infty}^{x_2}f(t)dt - \\int_{-\\infty}^{x_1}f(t)dt = \\int_{x_1}^{x_2}f(t)dt$\n",
    "\n",
    "This means if $x_1 = x_2$, then the probability will be 0. Using this we can derive the following property\n",
    "- $P(X\\le x) = P(X<x)$.\n",
    "    - $P(X<x) = P(X\\le x) - P(X=x) = P(X\\le x) - 0 = P(X\\le x)$.\n",
    "\n",
    "### Conceptual\n",
    "A continuous random variable's pdf and a discrete random variable's pmf both models probability, however they do it in different ways. A pmf models probability directly, while a pdf models probability indirectly. The pdf models the probability density (hence the name) and the probability is the area under the curve. \n",
    "\n",
    "Probability density is analogous to density in physics. In physics, density is how much \"stuff\" is in a given unit volume. Objects with higher density has a higher mass, assuming they have the same volume. Probability density is how much probability is in a given unit interval. And values with a higher probability density is relatively more \"likely\" to occur, even though the absolute probability is $0$.\n",
    "\n",
    "The value of the pdf at a particular point is not constrained between $[0,1]$. Suppose the mass of object is constrain to be between $0$g and $1$g. The density an object is not constrain to be between $0$ and $1$. The object could have a high density (like $2$g), but have a low volume (like .1cm). Similarly, probability density could be greater than 1 even though probability is constrained to be between $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Value\n",
    "The definition of expected value in the continuos case is the following:\n",
    "- (4) $E(X)=\\int_{-\\infty}^{\\infty}xf(x)dx$\n",
    "\n",
    "It also share all of expected value properties for the discrete case:\n",
    "- $E(g(X)) = \\int_{-\\infty}^{\\infty}g(x)f(x)dx$\n",
    "    - This property was slightly modified. Summation was replaced with integration.\n",
    "- $E(c) = c$\n",
    "- $E(cX) = cE(X)$\n",
    "- $E(X + Y) = E(X) + E(Y)$\n",
    "\n",
    "## Variance\n",
    "The definition of variance in the continuos case remains the same and share the same properties.\n",
    "- $Var(X) = E[(X - \\mu)^2]$\n",
    "- $Var(c) = 0$\n",
    "- $Var(X + c) = Var(X)$\n",
    "- $Var(aX) = a^2Var(X)$\n",
    "- $Var(X + Y) = Var(X) + Var(Y)$, if $X$ and $Y$ are independent\n",
    "- $Var(aX + bY) = a^2Var(X) + b^2Var(Y) + abCov(X, Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Random Variable\n",
    "A uniform random variable describes an experiment where there is an arbitrary result and is bounded by a min and max value (inclusive or exclusive), . \n",
    "\n",
    "The pdf of the uniform is the following:\n",
    "- (5) $f(x) = \\begin{cases} \\frac{1}{a - b}, \\quad \\text{if } a\\le x \\le b \\\\ 0, \\quad \\text{o.w.} \\end{cases}$\n",
    "\n",
    "### Notation\n",
    "- $X \\sim Unif(a, b)$\n",
    "- $X \\sim U(a, b)$\n",
    "\n",
    "### Expected Value and Variance\n",
    "- (6) $E(X) = \\frac{a+b}{2}$\n",
    "- (7) $Var(X) = \\frac{(b-a)^2}{12}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal (Gaussian) Random Variable\n",
    "A normal random variable describes an experiment whose result has a high probability of being near the mean. The probability decreases rapidly the more the result deviate from the mean. It is symmetric around the mean with forms a bell shape curve ranging from $[-\\infty, \\infty]$\n",
    "\n",
    "The pdf of a normal distribution is the following:\n",
    "- (8) $f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$\n",
    "    - $exp(x) = e^x$\n",
    "\n",
    "### Notation\n",
    "- $X \\sim N(\\mu, \\sigma^2)$\n",
    "\n",
    "### Mean and Variance\n",
    "- (9) $E(X) = \\mu$\n",
    "- (10) $Var(X) = \\sigma^2$\n",
    "\n",
    "### Standard Normal\n",
    "A special instance of the Normal distribution is $N(0,1)$. All normal distributions are similar in shape, so all normal distributions can be converted into the standard normal using the following formula:\n",
    "- $Z = \\frac{X-\\mu}{\\sigma}$  \n",
    "    - $X \\sim N(\\mu, \\sigma^2)$  \n",
    "    - $Z \\sim N(0, 1)$  \n",
    "    \n",
    "The cdf of a standard normal is sometimes notated as $\\Phi(x)$\n",
    "\n",
    "### Properties\n",
    "- A linear combination of normal random variable will result in a normal distribution.\n",
    "- Binomial $B(n,p)$ is approximately normal $N(np, np(1-p)$, if $n$ is large is $p$ is not too close to 0 or 1\n",
    "    - $B(n,p) \\stackrel{.}{\\sim} N(np, np(1-p)$ \n",
    "- Poisson $Pois(\\lambda)$ is approximately normal $N(\\lambda, \\lambda)$ for large values of $\\lambda$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations  \n",
    "General  \n",
    "1) $f(x)=\\frac{dF(x)}{dx}$ wherever the derivative exists  \n",
    "2) $F(x)=\\int_{-\\infty}^{x}f(t)dt$  \n",
    "3) $P(x_1 \\le X \\le x_2) = \\int_{x_1}^{x_2}f(t)dt$   \n",
    "4) $E(X)=\\int_{-\\infty}^{\\infty}xf(x)dx$  \n",
    "\n",
    "Uniform  \n",
    "5) $f(x) = \\begin{cases} \\frac{1}{a - b}, \\quad \\text{if } a\\le x \\le b \\\\ 0, \\quad \\text{o.w.} \\end{cases}$  \n",
    "6) $E(X) = \\frac{a+b}{2}$  \n",
    "7) $Var(X) = \\frac{(b-a)^2}{12}$  \n",
    "\n",
    "Normal   \n",
    "8) $f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$  \n",
    "9) $E(X) = \\mu$  \n",
    "10) $Var(X) = \\sigma^2$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}