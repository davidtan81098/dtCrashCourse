{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistics\n",
    "## Chapter 1.6 - Function of Random Variables\n",
    "There are times where one random variable $Y$ will be defined as a function of another random variable $X$. If know the distribution of $X$ and not $Y$, we would need to calculate the distribution $Y$. For the rest of the chapter, suppose we have a random variable $X$, and random variable $Y$ where $Y=g(X)$ and $X=g^{-1}(Y)$. Since function $g$ is invertible, then $g$ is also monotonic (the derivative is always $\\le 0$ or $\\ge0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Case\n",
    "We need to model the probability associated with $Y$. We will do this by deriving the pmf of $Y$.\n",
    "\n",
    "The pdf of $Y$, $p_Y(y)$ is defined as the following:\n",
    "- $p_Y(y) = P(Y=y) = P(g(X) = y)$\n",
    "\n",
    "We know how the random variables $X$ and $Y$ are related. We can redefine the event inside the probability using $X$.  So we will subsitute $Y$ with $g(X)$.\n",
    "- $P(Y=y) = P(g(X) = y)$\n",
    "\n",
    "We can once again defined the event so that the probability statement matches the definition of the pmf of $X$, $p_X(x)$\n",
    "- $P(g(X) = y) = P(X = g^{-1}(y)) = p_X(g^{-1}(y))$\n",
    "\n",
    "To summarise what is above:\n",
    "- (1) $p_Y(y) = p_X(g^{-1}(y))$\n",
    "\n",
    "### Conceptual\n",
    "Imagine the event $X=x$ and event $Y=y$ are the results for the same set of outcomes $A$ and the sample space of all the outcomes is $S$. That means both $P(X=x)$ and $P(Y=y)$ both equal to $\\frac{|A|}{|S|}$, therefore they have the same probability. We know that the 2 random variables are related, $Y=g(X)$. That means result of one random variables can be rewritten in terms of the the other, $y=g(x)$ and $x = g^{-1}(y)$. We can intrept equation 1 as the probability at $x$ got \"moved\" to $g(x)$. Do this for a all values of $x$ and we will get the pmf of $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Case\n",
    "We need to model the probability associated with $Y$. We have to model the cdf of $Y$ because pdf models probability density, not probability.\n",
    "\n",
    "- You can also use cdf for the discrete case.\n",
    "\n",
    "### If $g' \\ge 0$\n",
    "The cdf of $Y$, $F_Y(y)$, is defined as the following:\n",
    "- $F_Y(y) = P(Y \\le y)$\n",
    "\n",
    "We know how the random variables $X$ and $Y$ are related. We can redefine the event inside the probability using $X$. So we will subsitute $Y$ with $g(X)$.\n",
    "- $P(Y \\le y) = P(g(X) \\le y)$\n",
    "\n",
    "We can once again defined the event so that the probability statement matches the definition of the cdf of $X$, $F_X(x)$\n",
    "- $P(g(X) \\le y) = P(X \\le g^{-1}(y)) = F_X(g^{-1}(y))$\n",
    "\n",
    "To summarise what is above:\n",
    "- (2) $F_Y(y) = F_X(g^{-1}(y))$\n",
    "\n",
    "To find the pdf of $Y$, $f_Y(y)$, we need to differentiate both sides of equation 2 with respect to $y$:\n",
    "- $f_Y(y) = \\frac{d}{dy}F_Y(y) = \\frac{d}{dy}F_X(g^{-1}(y)) = f_X(g^{-1}(y))\\frac{dg^{-1}(y)}{dy}$\n",
    "    - Do not forget to apply chain rule\n",
    "\n",
    "### If $g' \\le 0$\n",
    "The beginning steps remain the same:\n",
    "- $F_Y(y) = P(Y \\le y) = P(g(X) \\le y)$\n",
    "\n",
    "Since $g$ is decreasing, the inequality flips when we take the inverse:\n",
    "- $P(g(X) \\le y) = P(X \\ge g^{-1}(y)) = 1 - P(X < g^{-1}(y))$\n",
    "\n",
    "We know for a continuous distribution $P(X < x) = P(X \\le x)$, we can add the equality part back in:\n",
    "- $1 - P(X < g^{-1}(y)) = 1 - P(X \\le g^{-1}(y)) = 1 - F_X(g^{-1}(y))$\n",
    "\n",
    "To summarise what is above:\n",
    "- (3) $F_Y(y) = 1 - F_X(g^{-1}(y))$\n",
    "\n",
    "To find the pdf, we need to differentiate both sides of equation 3:\n",
    "- $f_Y(y) = \\frac{d}{dy}F_Y(y) = \\frac{d}{dy}1-F_X(g^{-1}(y)) = -f_X(g^{-1}(y))\\frac{dg^{-1}(y)}{dy}$\n",
    "\n",
    "Recall that $\\frac{dg^{-1}(y)}{dy}$ is negative. If we factor out a negative sign, we get the following:\n",
    "- $-f_X(g^{-1}(y))\\frac{dg^{-1}(y)}{dy} = f_X(g^{-1}(y))\\frac{-dg^{-1}(y)}{dy}$\n",
    "\n",
    "Notice that pdf part matches the case where $g'\\ge 0$ and the chain rule part is positive in both cases. We can combine both pdf equations into the following:\n",
    "- (4) $f_Y(y) = f_X(g^{-1}(y)) \\Big| \\frac{dg^{-1}(y)}{dy} \\Big|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What If $g$ is Not Monotonic?\n",
    "\n",
    "If $g$ is not monotonic, such as $Y=X^2$, we can split the domain into different sections so that each individual section is monotonic. We do the transformation separately and combine the result.\n",
    "\n",
    "Example  \n",
    "The domain of $X$ can be split into $x>0$ and $x \\le 0$  \n",
    "For $x>0$, $g(x) = x^2, g^{-1}(x) = \\sqrt x$\n",
    "- $f_Y(y) = f_X(\\sqrt y) \\frac{1}{2\\sqrt y}, \\quad \\text{for }y>0$\n",
    "\n",
    "For $x \\le 0$, $g(x) = x^2, g^{-1}(x) = -\\sqrt x$\n",
    "- $f_Y(y) = f_X(-\\sqrt y) \\frac{1}{2\\sqrt y}, \\quad \\text{for }y\\ge0$\n",
    "\n",
    "Notice that they share region $y>0$ and it is continuous $y=0$. We can combine both halves into a single pdf and get the following:\n",
    "- $f_Y(y) = \\frac{1}{2\\sqrt y} (f_X(\\sqrt y) + f_X(-\\sqrt y)) , \\quad \\text{for }y\\ge0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square Random Variable\n",
    "A commonly used distribution that is derived from a function of random variables is the Chi-square. A Chi-square random variable is derived from a sum of squared standard normal random variables.\n",
    "- $\\chi^2_n$ or $\\chi^2(n)= \\sum_{i=1}^n X_i^2$\n",
    "    - $X_i \\sim N(0,1)$\n",
    "    - has $n$ \"degrees of freedom\"\n",
    "\n",
    "### Mean and Variance\n",
    "- $E(\\chi^2_n) = k$\n",
    "- $Var(\\chi^2_n) = 2k$\n",
    "\n",
    "### Properties\n",
    "If $X_1 \\sim \\chi^2_{n1}$ and $X_2 \\sim \\chi^2_{n2}$ and $X_1, X_2$ are indpendent, then the following is true:\n",
    "- $X_1 + X_2 \\sim \\chi^2_{n1+n2}$\n",
    "\n",
    "## T Distribution\n",
    "The t distribution is derived from a standard normal distribution, $Z$, and a Chi-square distribution, $W$, with $v$ degrees of freedom.\n",
    "- $T = \\frac{Z}{\\sqrt{W/v}}$\n",
    "\n",
    "This is basically a normal distribution with fatter tails. This is the sample distribution when the sample is small and the population is normally distributed. This arises from the fact the sample variance is also a random variable and has error bars associated with it. The t distribution converges onto the normal as $v$ approach $\\infty$. This means that for a large sample size, $T \\stackrel{.}{\\sim} N(0, 1)$ and normal distribution can be used to make inferences instead.\n",
    "\n",
    "## F Distribution\n",
    "The F distribution is a comparison between 2 chi-square distribution, $W_1, W_2$, with 2 degrees of freedom, $v_1, v_2$.\n",
    "- $F = \\frac{W_1/v_1}{W_2/v_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations\n",
    "pmf  \n",
    "1) $p_Y(y) = p_X(g^{-1}(y))$  \n",
    "\n",
    "cdf  \n",
    "2) $F_Y(y) = F_X(g^{-1}(y)), \\quad \\text{if } g' \\ge 0$  \n",
    "3) $F_Y(y) = 1 - F_X(g^{-1}(y)), \\quad \\text{if } g' \\le 0$  \n",
    "\n",
    "pdf  \n",
    "4) $f_Y(y) = f_X(g^{-1}(y)) \\Big| \\frac{dg^{-1}(y)}{dy} \\Big|$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}