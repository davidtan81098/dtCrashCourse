{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistics\n",
    "\n",
    "\n",
    "\n",
    "## Chapter 1.1 - Probability\n",
    "Statistics is the science of analyzing and intrepreting data. At the heart of statistics is probability. Typically, the goal of statistics is to be able to assign probability to questions of interest or to answer questions of interest using probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface\n",
    "This crash course is a not a replacement for a textbook. It does not cover every topic and it does not have practice problems. It will cover topics I think are important and/ or taught poorly by other textbooks. This is meant to be used as a supplement or for those who want a review.\n",
    "\n",
    "This crash course will emphasize conceptual understanding and equation derativation (to a lesser degree). I strongly dislike the common \"learning technique\"  of memorizing then plug and chug. I believe understanding things conceptually and seeing where they come from provides a more fundamental understanding of the subject than  memorizing definitions, equations, question style, and solving procedures. The fundamental understanding helps with applying it to real world senerios and not just carefully constructed problem sets.\n",
    "\n",
    "Equations will be introduced throughout the textbook. The most important ones are labeled with a number and compiled at the bottom of the chapter.\n",
    "\n",
    "We will always start off definitions and notations that are needed for this chapter. Only the definitions and notations discussed in previous chapters and those ssociated with the prerequisites not will be defined.\n",
    "\n",
    "### Prerequisites\n",
    "- Alegbra\n",
    "- Calculus\n",
    "- Linear Alegbra\n",
    "- Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions  \n",
    "- Random trial\n",
    "    - A proccess/ experiment with 2+ possible outcomes\n",
    "- Random variable\n",
    "    - The (numerical) result of a random trial\n",
    "- Sample point\n",
    "    - Any possible outcome from a random trial\n",
    "- Sample space\n",
    "    - The set of all possible sample points\n",
    "- Event \n",
    "    - A collection of sample point\n",
    "    - A subset from the sample space\n",
    "- Probability \n",
    "    - The ratio between the size of a desired event and the size of the sample space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "- Random variables\n",
    "    - Capital letters\n",
    "    - e.g. $X$, $Y$\n",
    "- Sets\n",
    "    - Capital letters\n",
    "    - e.g. $S$, $A$, $B$, $C$\n",
    "- Probability of an event\n",
    "    - Capital $P$ followed by the event surrounded by parentheses\n",
    "    - The event can be defined using a set\n",
    "    - The event can be defined using random variables, a particular outcome, and inequalities\n",
    "        - If the outcome notated with a lowercase letter if it is also a variable. This is different from a random variable because it is not from an experiment. This is a sample point in the sample space.\n",
    "    - e.g. $P(A)$, $P(B)$ $P(X = 9)$, $P(X \\le 9)$, $P(X = x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability\n",
    "\n",
    "Suppose we have an experiment. Let set $S$ be the sample space of the experiment and $X$ is the random variable for the experiment. Let set $A$, $B$ be a subset of $S$ that correspond to event $A$, $B$. Using our definition of probability, the equation for the probability of A or $P(A)$ is the following:\n",
    "- $P(A) = \\frac{|A|}{|S|}$  \n",
    "\n",
    "How $|A|$ and $|S|$ is calculated can be fairly complicated and nuanced, and will not be covered in this crash course.\n",
    "\n",
    "Based on the definition of event and sample space, we know the following:\n",
    "- (1) $P(S) = 1$\n",
    "- (2) $P(A) \\ge 0$\n",
    "\n",
    "We can also easily deduce the equation to find probability of the complement of $A$ or $P(A^c)$\n",
    "- (3) $P(A^c) = P(S) - P(A) = 1 - P(A)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of Set Intersection\n",
    "\n",
    "Suppose we want to find the probability of $A$ and $B$ occuring simultaneously or $P(A, B)$. This is the same as finding the interection of $A$ and $B$. \n",
    "- $P(A,B) = P(A \\cap B)$  \n",
    "- In statistics, we typically use the comma notation instead of using the interect symbol.\n",
    "\n",
    "If $A$ and $B$ has no intersection, then $P(A,B) = 0$. In other words, $A$ and $B$ are mutually exclusive.  \n",
    "\n",
    "#### Conditional Probability\n",
    "Suppose we have 2 events $A$ and $B$. Also, suppose that event $B$ has already occurred and we know the value. Then the probability of $A$ can be recalculated using the new information from $B$. This is called the conditional probability of $A$ given $B$ or normally shorten to probability of $A$ given $B$. The notation for \"given\" is using a vertical bar. Everything right of the bar are known values. e.g. $P(A|B), P(A|B,C), P(A,B|C)$\n",
    "\n",
    "Initially, it may sound odd to have events occur at different times and knowing one and not the other. Here is a real life example of conditional probability. \n",
    "\n",
    "Example  \n",
    "You are a doctor and you have a new patient, Alex. Alex is thinks he/ she may be color blind. The probability of a person being color blind is roughly 4.4%. However, men are more likely to be color due to biology. You look at the patient chart and see that Alex is male. The probability of men being color blind is roughly 8.3% and the probability of women being color blind is 0.5%. Since Alex is male, his probability of being color blind is now 8.3%.  \n",
    "- When we did not know the gender of Alex, the probability of being color blind is 4.6%\n",
    "- If Alex was male, then the probability of being color blind is 8.3%\n",
    "- If Alex was female, then the probability of being color is 0.5%\n",
    "\n",
    "An easy way to think about conditional probability is what is the probability something in set $B$ also has set $A$. The sample space of this question is no longer set $S$, but set $B$ instead. This is because everything in set $B^c$ is no longer relevant to the question. Using the definition of probability, we can derive the following equation:\n",
    "- (4) $P(A|B)= \\frac{|A, B|}{|B|} = \\frac{|A, B|/|S|}{|B|/|S|} = \\frac{P(A,B)}{P(B)}$\n",
    "\n",
    "If $P(A|B)=P(A)$, then $A$ and $B$ are said to be independent (otherwise it said to be dependent). This means having knowledge of $B$ does not influence the probability of $A$. \n",
    "\n",
    "We can rearrange equation 4 so we can decompose $P(A,B)$\n",
    "- (5) $P(A, B) = P(A|B) P(B)$\n",
    "- This is called the _Multiplicative Law of Probability_\n",
    "\n",
    "If $A$ and $B$ are independent then equation 5 simplifies to:\n",
    "- (6) $P(A,B) = P(A)P(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of Set Union\n",
    "\n",
    "Suppose set $C = A \\cup B$. What elements are in set $C$? If we draw a venn diagram, we can see that if we add the elements of set $A$ and $B$ together, then $A \\cap B$ is counted twice. Using this knowledge, we can derive the following:\n",
    "- (7) $P(A \\cup B) = P(A) + P(B) - P(A, B)$\n",
    "- This is called the _Additive Law of Probability_\n",
    "- Using the same idea, you can derive the equation for the probability 3+ sets union together.\n",
    "\n",
    "If $A$ and $B$ are mutually exclusive, then $P(A,B) = 0$. Then equation 7 will simplify to the following:\n",
    "-  (8) $P(A \\cup B) = P(A) + P(B)$  \n",
    "\n",
    "Suppose $S$ is broken into $n$ mutually exclusive subsets, $A_1, A_2, A_3, ..., A_n$. Then using equation 8, we have the following:\n",
    "- (9) $P(S) = P(A_1 \\cup A_2 \\cup A_3 \\cup ... \\cup A_n) = \\sum_{i=1}^n P(A_i)=1$\n",
    "\n",
    "Now suppose we also have set $B$. This implies that $B \\cap A_i$ for $1 \\ge i \\le n$ are also mutually exclusive. Using equation 8 again, we have the following equation:\n",
    "- $P(B) = P((B \\cap A_1) \\cup (B \\cap A_2) \\cup ... \\cup (B \\cap A_n)) = \\sum_{i=1}^n P(B, A_i)$\n",
    "\n",
    "Now applying the Multiplicative Law of Probability, we get following equation:\n",
    "- (10) $P(B) = \\sum_{i=1}^n P(B| A_i)P(A_i)$\n",
    "- This is called the _Law of Total Probability_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "By applying the Multiplicative Law to equation 4, we get the following :\n",
    "- (11) $P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$\n",
    "- This is called Bayes theorem\n",
    "\n",
    "This is useful because sometimes it might be very difficult to get data on $P(A|B)$ and easy to get data $P(B|A)$.\n",
    "\n",
    "We can use the color blindness example again. Let $A$ be the patient is male and $B$ be the patient is color blind. Suppose we know the following:\n",
    "- $P(B|A) = .083$\n",
    "- $P(B) = 0.044$\n",
    "- $P(A) = 0.5$\n",
    "\n",
    "To find $P(A|B)$ or the probability a color bind patient is male, we apply Bayes rule and get $\\frac{0.083 * 0.5}{0.044} = 0.943$. Notice that we did not know the probability intially, but by using the data we do have, we can calculate it still.\n",
    "\n",
    "We can generalize Bayes theorem even future by applying both Multiplicative Law and the Total Law of Probability to get the following:  \n",
    "- (12) $P(A|B)=\\frac{P(B|A)P(A)}{\\sum_{i=1}^n P(B|C_i)P(C_i)}$, where $C_i$ are mutually exclusive for $1 \\le i \\le n$ and the union of all $C_i = S$\n",
    "- Note: $A$ could be one of the $C_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations\n",
    "General  \n",
    "1) $P(S) = 1$  \n",
    "2) $P(A) \\ge 0$  \n",
    "3) $P(A^c) = P(S) - P(A) = 1 - P(A)$  \n",
    "4) $P(A|B) = \\frac{P(A,B)}{P(B)}$  \n",
    "9) $P(S) = \\sum_{i=1}^n P(A_i)=1$  \n",
    "\n",
    "Multiplicative Law of Probability  \n",
    "5) $P(A, B) = P(A|B) P(B)$   \n",
    "6) $P(A,B) = P(A)P(B)$, if $A,B$ are independent  \n",
    "\n",
    "Additive Law of Probability  \n",
    "7) $P(A \\cup B) = P(A) + P(B) - P(A, B)$  \n",
    "8) $P(A \\cup B) = P(A) + P(B)$, if $A,B$ are mutually exclusive  \n",
    "\n",
    "Law of Total Probability  \n",
    "10) $P(B) = \\sum_{i=1}^n P(B| A_i)P(A_i)$  \n",
    "\n",
    "Bayes Theorem  \n",
    "11) $P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$  \n",
    "12) $P(A|B)=\\frac{P(B|A)P(A)}{\\sum_{i=1}^n P(B|C_i)P(C_i)}$, where $C_i$ are mutually exclusive for $1 \\le i \\le n$ and the union of all $C_i = S$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}