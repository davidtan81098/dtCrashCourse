{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Introduction to Statistics\n",
    "\n",
    "## Chapter 2.1 - Point Estimate\n",
    "\n",
    "A parameter is a property or feature that describes the entire population. Typically, the parameter of interest is a parameter of a distribution (such as  $\\lambda$ for Poisson). A point estimate is a statistic that tries to estimate the parameter."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Properties of a estimation\n",
    "Suppose we have have dataset $X$, and we want to estimate parameter $\\theta$. We will try to estimate it using $\\theta$ using some sort of statistic, $\\hat\\theta$ (the hat usually refers to an estimator). How do we tell if $\\hat\\theta$ is a good estimator for $\\theta$? We can look at the properties of the estimator. \n",
    "\n",
    "### Bias\n",
    "$Bias(\\hat\\theta)=E(\\hat\\theta) - \\theta$\n",
    "\n",
    "Generally, we want an unbiased estimator, $Bias(\\hat\\theta) = 0$. In other words, $E(\\hat\\theta)=\\theta$\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "$MSE(\\hat\\theta) = E[(\\hat\\theta - \\theta)^2]$\n",
    "\n",
    "Mean squared error is one way of measuring how much out estimate deviate from the actual parameter. The MSE has an interesting property in which it can be decomposed into the following:\n",
    "\n",
    "$MSE(\\hat\\theta) =  Var(\\hat\\theta) + Bias(\\hat\\theta)^2 $\n",
    "- $MSE(\\hat\\theta) = E[(\\hat\\theta - \\theta)^2] = E(\\hat\\theta^2) - 2\\theta E(\\hat\\theta) + \\theta^2 = E(\\hat\\theta^2) - 2\\theta E(\\hat\\theta) + \\theta^2 - E(\\hat\\theta)^2 + E(\\hat\\theta)^2$\n",
    "- $= [E(\\hat\\theta^2)- E(\\hat\\theta)^2] + [E(\\hat\\theta)^2 - 2\\theta E(\\hat\\theta) + \\theta^2] = Var(\\hat\\theta) + [E(\\hat\\theta) - \\theta]^2 =  Var(\\hat\\theta) + Bias(\\hat\\theta)^2$\n",
    "\n",
    "Generally, we want to minimize the MSE of an estimator. From the decomposition, we can see when we minimize the MSE, either the variance, the bias, or both components are being minimized. That means we can have situations where an unbiased estimator has a higher MSE than a baised estimator. Which estimator is better will depend on the situation, the difference in the MSE, and how large is the bias.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Consistency\n",
    "\n",
    "Suppose we are calculating $\\hat\\theta$, and suppose $\\hat\\theta_n$ means that we calculated $\\hat\\theta$ using $n$ number of datapoints. We hope that if we use more data to calculate $\\hat\\theta$, the more likely that resulting value is closer to the true $\\theta$. If this is true, then the estimate is consistent.\n",
    "\n",
    "The formal definition of consistency is to say $\\hat\\theta_n$ converges in probability to $\\theta$.  \n",
    "$lim_{n \\rightarrow \\infty} P(|\\hat\\theta_n-\\theta| > \\epsilon) = 0$,  where $\\epsilon$ is any positive number.  \n",
    "Which is sometimes notated as $\\hat\\theta_n \\stackrel{P}{\\rightarrow} \\theta$\n",
    "\n",
    "In loose terms, the estimator $\\hat\\theta$ is consistent when $\\hat\\theta$ becomes a more accurate estimation for $\\theta$ as the sample size increases. You can conceptualize it as if we use the entire population as the sample, then $\\hat\\theta = \\theta$ (technically only true if the \"population\" is infinite in size e.g. flipping a coin). \n",
    "\n",
    "### Unbaised consistent estimator\n",
    "Theorem:  \n",
    "If $\\hat\\theta$ is unbaised and $lim_{n \\rightarrow \\infty} Var(\\hat\\theta)=0$, then $\\hat\\theta$ is consistent.\n",
    "\n",
    "### Biased consistent estimator \n",
    "The theorem only provides a condition where a estimator can be consistent. They are not a requirement. An estimator can be consistent even if the estimator is biased. For example, the biased form of the sample variance, $\\frac{1}{n} \\sum_{i=1}^n(y_i-\\bar{y}^2)$, is a consistent estimator for variance.\n",
    "\n",
    "\n",
    "### Law of Large Numbers\n",
    "The Law of Large Numbers state that the sample mean will approach the true mean as the sample size increase. This can be translated that the sample mean is a consistent estimator for the true mean. We can show this is true by using the theorem above. \n",
    "\n",
    "Assume $E(Y) = \\mu, Var(Y)=\\sigma^2$  \n",
    "- $E(\\bar Y) = \\frac{1}{n} E(Y_1 + Y_2 + Y_3 + ... + Y_n) = \\frac{1}{n} n \\mu = \\mu$\n",
    "- $Var(\\bar Y) = \\frac{1}{n^2} Var(Y_1 + Y_2 + Y_3 + ... + Y_n) = \\frac{1}{n^2} Var(Y_1 + Y_2 + Y_3 + ... + Y_n) = \\frac{1}{n^2} n \\sigma^2$ = \\frac{\\sigma^2}{n}\n",
    "- $lim_{n \\rightarrow \\infty} Var(\\bar Y) = lim_{n \\rightarrow \\infty} \\frac{\\sigma^2}{n} = 0$\n",
    "- Since $\\bar Y$ is unbaised and $lim_{n \\rightarrow \\infty} Var(\\bar Y)=0$, then $\\bar Y$ is consistent estimator for $\\mu$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Estimation Methods\n",
    "\n",
    "When we collect the data and create a histogram, we can see the distribution of the sample data. By looking at the shape, we can make an assumption on what is the distribution. Now, we can estimate the parameters for the distribution.  \n",
    "\n",
    "### Method of Moment (MOM)\n",
    "The $k^{th}$ moment of a random variable is defined to be:  \n",
    "$\\mu'_k = E(Y^k)$   \n",
    "The $k^{th}$ moment of a sample is defined to be:  \n",
    "$m'_k = \\frac{1}{n}\\sum_{i=1}^n(Y^k)$   \n",
    "\n",
    "Moments are properties of a random variable and sample moments are properties of the sample. The method of moment tries to estimate the parameters by setting $\\mu'_k = m'_k$. Basically, it is trying to estimate the parameters by making sure the properties match. \n",
    "\n",
    "First, you will need to calculate $n$ moments in terms of the parameters, where as $n=$ number of parameters. Then, you will need to calculate $n$ sample moment and set it equal to the corresponding population moment. Finally, solve the system of equations.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "The MLE method estimates the parameters by maximizing the probability (density) of observing the data. Conceptually, this means that you want high occuring results to have a high probability and low occuring results to have a low probability.\n",
    "\n",
    "Imagine the probability of observing the data is $P(y_1, y_2, ..., y_n)$. We assume that the data is iid, then the probability can be simplified to $P(y_1)P(y_2)...P(y_n)$. This equation is called the likelihood function. How is this different from a regular joint distribution function? Almost nothing. When we say likelihood function, it means that we have to solve for the parameters. Meanwhile, when we say distribution function, we assume we have the parameters. The likelihood function is usually notated like  $L(y_1, y_2, ..., y_n|\n",
    "\\theta_1, \\theta_2, ..., \\theta_m)$ or $L(\\theta_1, \\theta_2, ..., \\theta_m)$, where as $\\theta$s is the unknown parameter.\n",
    "\n",
    "First, find the likelihood function $L(\\theta) = P(y_1)P(y_2)...P(y_n)$. Generally speaking, this equation is too difficult to solve for the maximun value of $\\theta$. So normally we solve for the log-likelihood $l(\\theta)$ instead, where as $l(\\theta) = log(L(\\theta))$. Since logorithms are a monotonic function, the $\\theta$ that maximizes the log-likelihood function is the same $\\theta$ that maximizes the likelihood function. Usually calculus is used to find the maximum value of $\\theta$. However, sometimes there is no closed form solution. Then other optimization techniques are used, such as Newton-Raphson iteration, gradient descent, etc. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Pros and Cons\n",
    "\n",
    "#### MOM\n",
    "- Pros  \n",
    "    - Yields consistant estimators\n",
    "    - \"Easy\" to calculate\n",
    "- Cons\n",
    "    - Estimates can be outside of the parameter space, especially with small samples\n",
    "    - Estimators can be biased\n",
    "\n",
    "### MLE\n",
    "- Pros\n",
    "    - Estimates are always in the parameter space\n",
    "    - Estimates are asymptotically normally distributed\n",
    "    - Yields Consistant estimators\n",
    "    - Smaller variance compared to MOM \n",
    "- Cons\n",
    "    - Estimators can be biased\n",
    "    - Can be (extremely) difficult to solve\n",
    "\n",
    "Because they have many good properties, MLEs are often preferred over MOMs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Checking Assumption\n",
    "\n",
    "Recall, the first step is to guess what is the distribution of the sample. Now that we have our estimates, we can check if we guessed the right distribution. A naive approach is to overlay the histogram of the sample data with the pdf curve. A better approach is to plot the quantiles of the sample data against the theoretical quantiles. If the distribution match, then the points on the plot will approximately lie on the $y=x$ line. Some deviation is to be expected, especially on the tail ends of the distribution and/or if the sample is sample. How much deviation is too much is fairly subjective."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}